{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝의 이론과 실제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝\n",
    "* 텍스트를 분석을 위한 데이터로 변경하는 것을 의미한다.\n",
    "* 일정한 길이의 vector로 변환 후 머신러닝 (딥러닝) 기법을 적용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝 방법\n",
    "* NLP(자연어처리) 기본도구\n",
    "    * Tokenize(낱말분석), stemming(형태소분석), lemmatize(표제어 추출)\n",
    "    * Chunking(묶음만들기)\n",
    "    * BOW(Bag of word, 단어가 얼마나 많이 삽입되었는지), TFIDF(Term Frequency-Inverse Document Frequency, 문서의 빈도와 역문서의 빈도) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝 도구 - 파이썬\n",
    "* NLTK \n",
    "    * 가장 널리 알려진 NLP 라이브러리\n",
    "* Scikit Learn \n",
    "    * 머신러닝 라이브러리, 기본적 NLP, 다양한 텍스트 마이닝 관련 지원\n",
    "* Gensim\n",
    "    * Word2Vec으로 유명하며 sklearin과 마찬가지로 텍스트 마이닝 도구 지원\n",
    "* Keras\n",
    "    * RNN, seq2seq 등의 딥러닝 위주의 라이브러리 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝 기본도구\n",
    "- 목적 : document, sentence 등을 sparse vector로 변환\n",
    "* Tokenize\n",
    "    * 대상이 되는 문서/문장을 최소 단위로 쪼갬\n",
    "* Text normalization\n",
    "    * 최소 단위 표준화\n",
    "* Pos-tagging\n",
    "    * 최소 의미 단위 품사 부착\n",
    "* Chunking\n",
    "    * 위 포즈태깅의 결과를 말모듬으로 다시 합침\n",
    "* BOW, TFIDF\n",
    "    * tokenized 결과를 이용하여 문서를 vector로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트마이닝 실습코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "from IPython.display import Image\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = 'C:/Users/ajou/Documents/chromedriver.exe' #구글드라이버 설치한 경로 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Come & Go', 'Mood', 'Savage Love (Laxed - Siren Beat)', 'Breaking Me', 'Laugh Now Cry Later', 'ily (i love you baby)', 'Holy', 'I Hope', 'Roses (Imanbek Remix)', 'Before You Go', 'POPSTAR', 'Lemonade', 'Bang!', 'Be Like That', 'Go Crazy', 'ROCKSTAR', 'If The World Was Ending', 'Blinding Lights', 'U 2 Luv', 'RNB']\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.shazam.com/ko/charts/top-200/united-states\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 빈 리스트 변수\n",
    "title_list = []\n",
    "\n",
    "for i in range(1, 21): \n",
    "    title = WebDriverWait(driver, 20) \\\n",
    "        .until(EC.presence_of_element_located((By.XPATH, f\"//*[@id='/charts/top-200/united-states']/div[2]/div[2]/div[1]/ul/li[{i}]/article/div[2]/div[1]/div[1]/a\")))\n",
    "    title_list.append(title.text)\n",
    "    \n",
    "print(title_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * TOKENIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come & Go Mood Savage Love (Laxed - Siren Beat) Breaking Me Laugh Now Cry Later ily (i love you baby) Holy I Hope Roses (Imanbek Remix) Before You Go POPSTAR Lemonade Bang! Be Like That Go Crazy ROCKSTAR If The World Was Ending Blinding Lights U 2 Luv RNB\n"
     ]
    }
   ],
   "source": [
    "text=title_list\n",
    "print(' '.join(text))\n",
    "text=(' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text Noramlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern = '[^\\w\\s]'\n",
    "repl =''\n",
    "text=re.sub(pattern=pattern, repl=repl, string=text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text=pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('come', 'VBN', 'O'),\n",
      " ('go', 'VBP', 'O'),\n",
      " ('mood', 'JJ', 'O'),\n",
      " ('savage', 'NN', 'O'),\n",
      " ('love', 'VB', 'O'),\n",
      " ('laxed', 'JJ', 'O'),\n",
      " ('siren', 'NNS', 'O'),\n",
      " ('beat', 'VBP', 'O'),\n",
      " ('breaking', 'VBG', 'O'),\n",
      " ('me', 'PRP', 'O'),\n",
      " ('laugh', 'IN', 'O'),\n",
      " ('now', 'RB', 'O'),\n",
      " ('cry', 'VBP', 'O'),\n",
      " ('later', 'RB', 'O'),\n",
      " ('ily', 'RB', 'O'),\n",
      " ('i', 'VB', 'O'),\n",
      " ('love', 'VBP', 'O'),\n",
      " ('you', 'PRP', 'O'),\n",
      " ('baby', 'VBP', 'O'),\n",
      " ('holy', 'JJ', 'O'),\n",
      " ('i', 'NNS', 'O'),\n",
      " ('hope', 'VBP', 'O'),\n",
      " ('roses', 'NNS', 'O'),\n",
      " ('imanbek', 'JJ', 'O'),\n",
      " ('remix', 'NN', 'O'),\n",
      " ('before', 'IN', 'O'),\n",
      " ('you', 'PRP', 'O'),\n",
      " ('go', 'VBP', 'O'),\n",
      " ('popstar', 'JJ', 'O'),\n",
      " ('lemonade', 'NN', 'O'),\n",
      " ('bang', 'NN', 'O'),\n",
      " ('be', 'VB', 'O'),\n",
      " ('like', 'IN', 'O'),\n",
      " ('that', 'DT', 'O'),\n",
      " ('go', 'VBP', 'O'),\n",
      " ('crazy', 'JJ', 'O'),\n",
      " ('rockstar', 'NN', 'O'),\n",
      " ('if', 'IN', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('world', 'NN', 'O'),\n",
      " ('was', 'VBD', 'O'),\n",
      " ('ending', 'VBG', 'O'),\n",
      " ('blinding', 'VBG', 'O'),\n",
      " ('lights', 'NNS', 'O'),\n",
      " ('u', 'JJ', 'O'),\n",
      " ('2', 'CD', 'O'),\n",
      " ('luv', 'NN', 'O'),\n",
      " ('rnb', 'NN', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "\n",
    "iob_tagged = tree2conlltags(text)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'come': 1,\n",
       "         'go': 3,\n",
       "         'mood': 1,\n",
       "         'savage': 1,\n",
       "         'love': 2,\n",
       "         'laxed': 1,\n",
       "         'siren': 1,\n",
       "         'beat': 1,\n",
       "         'breaking': 1,\n",
       "         'me': 1,\n",
       "         'laugh': 1,\n",
       "         'now': 1,\n",
       "         'cry': 1,\n",
       "         'later': 1,\n",
       "         'ily': 1,\n",
       "         'i': 2,\n",
       "         'you': 2,\n",
       "         'baby': 1,\n",
       "         'holy': 1,\n",
       "         'hope': 1,\n",
       "         'roses': 1,\n",
       "         'imanbek': 1,\n",
       "         'remix': 1,\n",
       "         'before': 1,\n",
       "         'popstar': 1,\n",
       "         'lemonade': 1,\n",
       "         'bang': 1,\n",
       "         'be': 1,\n",
       "         'like': 1,\n",
       "         'that': 1,\n",
       "         'crazy': 1,\n",
       "         'rockstar': 1,\n",
       "         'if': 1,\n",
       "         'the': 1,\n",
       "         'world': 1,\n",
       "         'was': 1,\n",
       "         'ending': 1,\n",
       "         'blinding': 1,\n",
       "         'lights': 1,\n",
       "         'u': 1,\n",
       "         '2': 1,\n",
       "         'luv': 1,\n",
       "         'rnb': 1})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = Counter(text)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'love',\n",
       " 'i',\n",
       " 'you',\n",
       " 'come',\n",
       " 'mood',\n",
       " 'savage',\n",
       " 'laxed',\n",
       " 'siren',\n",
       " 'beat',\n",
       " 'breaking',\n",
       " 'me',\n",
       " 'laugh',\n",
       " 'now',\n",
       " 'cry',\n",
       " 'later',\n",
       " 'ily',\n",
       " 'baby',\n",
       " 'holy',\n",
       " 'hope',\n",
       " 'roses',\n",
       " 'imanbek',\n",
       " 'remix',\n",
       " 'before',\n",
       " 'popstar',\n",
       " 'lemonade',\n",
       " 'bang',\n",
       " 'be',\n",
       " 'like',\n",
       " 'that',\n",
       " 'crazy',\n",
       " 'rockstar',\n",
       " 'if',\n",
       " 'the',\n",
       " 'world',\n",
       " 'was',\n",
       " 'ending',\n",
       " 'blinding',\n",
       " 'lights',\n",
       " 'u',\n",
       " '2',\n",
       " 'luv',\n",
       " 'rnb']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word.encode(\"utf8\").decode(\"utf8\"): ii for ii, word in enumerate(vocab,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'go': 1,\n",
       " 'love': 2,\n",
       " 'i': 3,\n",
       " 'you': 4,\n",
       " 'come': 5,\n",
       " 'mood': 6,\n",
       " 'savage': 7,\n",
       " 'laxed': 8,\n",
       " 'siren': 9,\n",
       " 'beat': 10,\n",
       " 'breaking': 11,\n",
       " 'me': 12,\n",
       " 'laugh': 13,\n",
       " 'now': 14,\n",
       " 'cry': 15,\n",
       " 'later': 16,\n",
       " 'ily': 17,\n",
       " 'baby': 18,\n",
       " 'holy': 19,\n",
       " 'hope': 20,\n",
       " 'roses': 21,\n",
       " 'imanbek': 22,\n",
       " 'remix': 23,\n",
       " 'before': 24,\n",
       " 'popstar': 25,\n",
       " 'lemonade': 26,\n",
       " 'bang': 27,\n",
       " 'be': 28,\n",
       " 'like': 29,\n",
       " 'that': 30,\n",
       " 'crazy': 31,\n",
       " 'rockstar': 32,\n",
       " 'if': 33,\n",
       " 'the': 34,\n",
       " 'world': 35,\n",
       " 'was': 36,\n",
       " 'ending': 37,\n",
       " 'blinding': 38,\n",
       " 'lights': 39,\n",
       " 'u': 40,\n",
       " '2': 41,\n",
       " 'luv': 42,\n",
       " 'rnb': 43}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'go',\n",
       " 1: 'love',\n",
       " 2: 'i',\n",
       " 3: 'you',\n",
       " 4: 'come',\n",
       " 5: 'mood',\n",
       " 6: 'savage',\n",
       " 7: 'laxed',\n",
       " 8: 'siren',\n",
       " 9: 'beat',\n",
       " 10: 'breaking',\n",
       " 11: 'me',\n",
       " 12: 'laugh',\n",
       " 13: 'now',\n",
       " 14: 'cry',\n",
       " 15: 'later',\n",
       " 16: 'ily',\n",
       " 17: 'baby',\n",
       " 18: 'holy',\n",
       " 19: 'hope',\n",
       " 20: 'roses',\n",
       " 21: 'imanbek',\n",
       " 22: 'remix',\n",
       " 23: 'before',\n",
       " 24: 'popstar',\n",
       " 25: 'lemonade',\n",
       " 26: 'bang',\n",
       " 27: 'be',\n",
       " 28: 'like',\n",
       " 29: 'that',\n",
       " 30: 'crazy',\n",
       " 31: 'rockstar',\n",
       " 32: 'if',\n",
       " 33: 'the',\n",
       " 34: 'world',\n",
       " 35: 'was',\n",
       " 36: 'ending',\n",
       " 37: 'blinding',\n",
       " 38: 'lights',\n",
       " 39: 'u',\n",
       " 40: '2',\n",
       " 41: 'luv',\n",
       " 42: 'rnb'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word = {ii: word for ii, word in enumerate(vocab)}\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위와 같이 전처리한 데이터를 바탕으로 BOW나 TFIDF 방식중 원하는 것으로 vector로 변환하여 텍스트 마이닝을 진행한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
